{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12103690,"sourceType":"datasetVersion","datasetId":7619911},{"sourceId":12127960,"sourceType":"datasetVersion","datasetId":7636924}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nlpaug","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:00:33.422436Z","iopub.execute_input":"2025-07-02T07:00:33.422711Z","iopub.status.idle":"2025-07-02T07:00:38.739788Z","shell.execute_reply.started":"2025-07-02T07:00:33.422681Z","shell.execute_reply":"2025-07-02T07:00:38.738772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification , AutoTokenizer,AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nfrom torch.utils.data import WeightedRandomSampler\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nimport warnings\nwarnings.filterwarnings(\"default\")\nimport nlpaug.augmenter.word as naw\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:00:38.741649Z","iopub.execute_input":"2025-07-02T07:00:38.741933Z","iopub.status.idle":"2025-07-02T07:01:57.746964Z","shell.execute_reply.started":"2025-07-02T07:00:38.741906Z","shell.execute_reply":"2025-07-02T07:01:57.746276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teacher_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\nteacher_model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:01:57.747837Z","iopub.execute_input":"2025-07-02T07:01:57.748598Z","iopub.status.idle":"2025-07-02T07:02:02.683899Z","shell.execute_reply.started":"2025-07-02T07:01:57.748574Z","shell.execute_reply":"2025-07-02T07:02:02.683133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_excel(\"/kaggle/input/d/narvindalt/something/AG - AI 2.xlsx\")\n\ndata[\"text\"] = data[\"Email Subject\"].fillna(\"\") + \" \" + \\\n               data[\"Email Query Discerption\"].fillna(\"\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:02.684838Z","iopub.execute_input":"2025-07-02T07:02:02.685163Z","iopub.status.idle":"2025-07-02T07:02:04.837305Z","shell.execute_reply.started":"2025-07-02T07:02:02.685131Z","shell.execute_reply":"2025-07-02T07:02:04.836263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:04.840130Z","iopub.execute_input":"2025-07-02T07:02:04.840471Z","iopub.status.idle":"2025-07-02T07:02:04.853819Z","shell.execute_reply.started":"2025-07-02T07:02:04.840445Z","shell.execute_reply":"2025-07-02T07:02:04.852831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2 = data.drop([\"Query Category\"],axis=1)\ndata2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:04.854849Z","iopub.execute_input":"2025-07-02T07:02:04.855170Z","iopub.status.idle":"2025-07-02T07:02:05.132898Z","shell.execute_reply.started":"2025-07-02T07:02:04.855143Z","shell.execute_reply":"2025-07-02T07:02:05.131945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2['Query Item'] = data2['Query Item'].str.split('-').str[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:05.133922Z","iopub.execute_input":"2025-07-02T07:02:05.134690Z","iopub.status.idle":"2025-07-02T07:02:05.145942Z","shell.execute_reply.started":"2025-07-02T07:02:05.134657Z","shell.execute_reply":"2025-07-02T07:02:05.145106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:05.146922Z","iopub.execute_input":"2025-07-02T07:02:05.147238Z","iopub.status.idle":"2025-07-02T07:02:05.169119Z","shell.execute_reply.started":"2025-07-02T07:02:05.147214Z","shell.execute_reply":"2025-07-02T07:02:05.168290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata2[\"Query Item\"] = le.fit_transform(data2[\"Query Item\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:05.170003Z","iopub.execute_input":"2025-07-02T07:02:05.170317Z","iopub.status.idle":"2025-07-02T07:02:05.188259Z","shell.execute_reply.started":"2025-07-02T07:02:05.170292Z","shell.execute_reply":"2025-07-02T07:02:05.187265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:05.189671Z","iopub.execute_input":"2025-07-02T07:02:05.190004Z","iopub.status.idle":"2025-07-02T07:02:05.203185Z","shell.execute_reply.started":"2025-07-02T07:02:05.189973Z","shell.execute_reply":"2025-07-02T07:02:05.202447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(x=data2[\"Query Item\"])\nplt.xticks(rotation=90)\nplt.title(\"Category Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:05.204062Z","iopub.execute_input":"2025-07-02T07:02:05.204390Z","iopub.status.idle":"2025-07-02T07:02:05.917651Z","shell.execute_reply.started":"2025-07-02T07:02:05.204344Z","shell.execute_reply":"2025-07-02T07:02:05.916745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nimport pandas as pd\nfrom collections import Counter\n\naugmenter = naw.ContextualWordEmbsAug(\n    model_path='bert-base-uncased', action=\"substitute\", device='cuda' if torch.cuda.is_available() else 'cpu'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:05.918862Z","iopub.execute_input":"2025-07-02T07:02:05.919550Z","iopub.status.idle":"2025-07-02T07:02:09.509838Z","shell.execute_reply.started":"2025-07-02T07:02:05.919526Z","shell.execute_reply":"2025-07-02T07:02:09.508884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_counts = data2[\"Query Item\"].value_counts()\nmax_count = label_counts.max()\n\nprint(label_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:09.510943Z","iopub.execute_input":"2025-07-02T07:02:09.511309Z","iopub.status.idle":"2025-07-02T07:02:09.522321Z","shell.execute_reply.started":"2025-07-02T07:02:09.511268Z","shell.execute_reply":"2025-07-02T07:02:09.521121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\naugmented_texts = []\naugmented_labels = []\n\nfor label, count in label_counts.items():\n    if count < max_count:\n        texts = data2[data2[\"Query Item\"] == label][\"text\"].tolist()\n        needed = max_count - count\n\n        repeats = needed // len(texts) + 1\n        texts_to_augment = (texts * repeats)[:needed]\n\n        print(f\"Augmenting class '{label}' with {needed} samples...\")\n        for text in tqdm(texts_to_augment, desc=f\"Augmenting '{label}'\", leave=False):\n            try:\n                aug_text = augmenter.augment(text)\n                augmented_texts.append(aug_text if isinstance(aug_text, str) else aug_text[0])\n                augmented_labels.append(label)\n            except Exception as e:\n                print(f\"Error augmenting: {text} â†’ {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T07:02:09.525640Z","iopub.execute_input":"2025-07-02T07:02:09.525928Z","execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aug_data = pd.DataFrame({\"text\": augmented_texts, \"Query Item\": augmented_labels})\ndata2 = pd.concat([data2, aug_data]).reset_index(drop=True)\n\nprint(data2[\"Query Item\"].value_counts())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2.to_csv(\"/kaggle/working/aug_df.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = list(data2[\"text\"])\ncategorys = list(data2[\"Query Item\"])\ntrue_labels = list(data2[\"Query Item\"].unique())\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    texts,\n    categorys,\n    test_size=0.2,\n    random_state=42,\n    stratify=categorys\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item['labels'] = torch.tensor(label)\n        return item","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = TextDataset(train_texts, train_labels, teacher_tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, teacher_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nteacher_model.to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=3, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\nloss_fn = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\nteacher_model.train()\noptimizer = AdamW(teacher_model.parameters(), lr=5e-5, weight_decay=0.0005)\n\nscheduler = ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.001, patience=1, verbose=True, min_lr=1e-50\n)\n\nearly_stopping = EarlyStopping(patience=10, min_delta=0.0001)\n\nfor epoch in range(2):\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        logits = outputs.logits\n        loss = loss_fn(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n        loop.set_postfix(loss=loss.item())\n\n    train_loss = total_loss / len(train_loader)\n    train_acc = accuracy_score(all_labels, all_preds)\n\n    teacher_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n            loss = loss_fn(logits, labels)\n\n            val_loss += loss.item()\n            preds = torch.argmax(logits, dim=1)\n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    val_loss /= len(val_loader)\n    val_acc = accuracy_score(val_labels, val_preds)\n\n    scheduler.step(val_loss)\n\n    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered.\")\n        break\n\n    teacher_model.train()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nstudent_tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-cased\")\nstudent_model = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-cased\", num_labels=42\n).to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nteacher_model.to(device)\nstudent_model.to(device)\n\nteacher_model.eval()\noptimizer = AdamW(student_model.parameters(), lr=5e-5,weight_decay=0.0005)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = TextDataset(train_texts, train_labels, student_tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, student_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef distillation_loss(student_logits, teacher_logits, true_labels, temperature=4.0, alpha=0.5):\n    # Soft targets\n    KD_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n        F.log_softmax(student_logits / temperature, dim=1),\n        F.softmax(teacher_logits / temperature, dim=1)\n    ) * (temperature ** 2)\n\n    # Hard labels\n    CE_loss = F.cross_entropy(student_logits, true_labels)\n\n    return alpha * KD_loss + (1 - alpha) * CE_loss\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 2\nteacher_model.eval()\nfor epoch in range(num_epochs):\n    student_model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", leave=False)\n    for batch in loop:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        with torch.no_grad():\n            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n            teacher_logits = teacher_outputs.logits\n\n        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n        student_logits = student_outputs.logits\n\n        loss = distillation_loss(student_logits, teacher_logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(student_logits, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n        loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n\n    train_accuracy = 100 * correct / total\n    print(f\"\\nEpoch {epoch+1}/{num_epochs} | Train Loss: {total_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n\n    student_model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", leave=False)\n    with torch.no_grad():\n        for batch in val_loop:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = F.cross_entropy(logits, labels)\n            val_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n            val_loop.set_postfix(val_loss=loss.item())\n\n    val_accuracy = 100 * val_correct / val_total\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%\\n\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, data_loader, device, desc=\"Evaluation\"):\n    model.eval()\n    model.to(device)\n    total_correct = 0\n    total_samples = 0\n    total_loss = 0\n\n    loop = tqdm(data_loader, desc=desc)\n    with torch.no_grad():\n        for batch in loop:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            loss = F.cross_entropy(logits, labels)\n            total_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n            total_correct += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n\n            loop.set_postfix(loss=loss.item(), acc=100 * total_correct / total_samples)\n\n    accuracy = 100 * total_correct / total_samples\n    avg_loss = total_loss / len(data_loader)\n    return accuracy, avg_loss\n\n\ntrain_dataset = TextDataset(train_texts, train_labels, teacher_tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, teacher_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nteacher_acc, teacher_loss = evaluate(teacher_model, val_loader, device)\n\nprint(f\"Final Teacher Accuracy: {teacher_acc:.2f}% | Loss: {teacher_loss:.4f}\")\n\n\ntrain_dataset = TextDataset(train_texts, train_labels, student_tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, student_tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=16)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\nstudent_acc, student_loss = evaluate(student_model, val_loader, device)\nprint(f\"Final Student Accuracy: {student_acc:.2f}% | Loss: {student_loss:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_model.save_pretrained(\"/kaggle/working/model\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:02:45.356Z"}},"outputs":[],"execution_count":null}]}